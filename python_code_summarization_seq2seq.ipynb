{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPAUvsVC9QCyyXwRsA6EqBT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AmerigoGiommetti/MLSA-code-summarization/blob/master/python_code_summarization_seq2seq.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4_gs33V5CsQO"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "ML-Based Python Code Summarization\n",
        "Course: Machine Learning for Software Analysis\n",
        "\n",
        "This project implements a machine learning model that generates\n",
        "natural language summaries from Python code snippets.\n",
        "\n",
        "Author: Amerigo Giommetti\n",
        "Academic Year: 2025/2026\n",
        "\"\"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# 1. Imports and Global Configuration\n",
        "# ============================================================\n",
        "\n",
        "#Tutte le librerie standard che serviranno per il progetto\n",
        "import random\n",
        "import math\n",
        "import time\n",
        "\n",
        "#Librerie necessarie per la manipolazione dei dati\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "#Librerie necessarie per il machine learning\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "#Libreria per il plotting e l'evaluation\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm as tdqm\n",
        "import nltk\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "\n",
        "#Scelta della CPU o della GPU per processare i dati\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {DEVICE}\")\n",
        "\n",
        "SEED = 42                 #Seed per la riproducibilità dei risultati\n",
        "BATCH_SIZE = 32           #Quanti dati si prendono alla volta\n",
        "EMBEDDING_DIM = 128       #Complessità della rappresentazione che il modello fa dei dati\n",
        "HIDDEN_DIM = 256          #Quantità di informazioni che il modello tiene ad ogni passaggio\n",
        "NUM_EPOCHS = 15           #Numero di volte che i dati vengono processati\n",
        "LEARNING_RATE = 1e-3      #Velocità con cui il modello impara\n",
        "MAX_CODE_LEN = 200        #Lunghezza massima degli input di codice\n",
        "MAX_SUMMARY_LEN = 75      #Lunghezza massima delle frasi summarization\n",
        "MAX_SAMPLES = 5000        #Lunghezza massima del dataset per il training"
      ],
      "metadata": {
        "id": "ndYyAI0p6WAi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# 2. Reproducibility and Utility Functions\n",
        "# ============================================================\n",
        "\n",
        "#Setta lo stesso seed per tutti i processi random del modello\n",
        "def set_seed(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "#Il seed che viene usato è quello definito sopra\n",
        "set_seed(SEED)\n",
        "\n",
        "#Helper function per l'aggiunta di padding negli input troppo corti\n",
        "def pad_sequence(seq, max_len, pad_value=0):\n",
        "    return seq[:max_len] + [pad_value] * max(0, max_len - len(seq))\n"
      ],
      "metadata": {
        "id": "m6yBz22-6XFR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# 3. Dataset Loading and Preprocessing\n",
        "# ============================================================\n",
        "\n",
        "#Primo dataset \"giocattolo\" per il test end to end della pipeline\n",
        "toy_data = [\n",
        "    (\"def add(x, y): return x + y\", \"returns the sum of two numbers\"),\n",
        "    (\"def sub(x, y): return x - y\", \"returns the difference of two numbers\"),\n",
        "    (\"def mul(x, y): return x * y\", \"returns the product of two numbers\"),\n",
        "]\n",
        "#Divisione del dataset in input e target\n",
        "codes, summaries = zip(*toy_data)\n",
        "\n",
        "# ===========================================================================\n",
        "# 3. Dataset Loading code_x_glue (Direct Download from S3/GitHub Source)\n",
        "# ===========================================================================\n",
        "\n",
        "#import per il dataset di huggingface\n",
        "from datasets import load_dataset\n",
        "#import per il train test split\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def load_data_final():\n",
        "    print(\"Caricamento dataset CodeXGLUE (Python)...\")\n",
        "    # Questo dataset è la versione pulita e stabile per Code Summarization\n",
        "    dataset = load_dataset(\"code_x_glue_ct_code_to_text\", \"python\")\n",
        "\n",
        "    # Prendiamo lo split di train e lo trasformiamo in DataFrame\n",
        "    train_data = dataset['train'].select(range(5000)) # Limite a 5000 per velocità\n",
        "\n",
        "    df = pd.DataFrame({\n",
        "        'code': train_data['code'],\n",
        "        'summary': train_data['docstring']\n",
        "    })\n",
        "\n",
        "    # Pulizia (come da Lecture 7)\n",
        "    df['code'] = df['code'].str.replace(r'\\s+', ' ', regex=True).str.strip()\n",
        "    df['summary'] = df['summary'].str.replace(r'\\s+', ' ', regex=True).str.strip()\n",
        "\n",
        "    return df\n",
        "\n",
        "train_df = load_data_final()\n",
        "print(f\"Dataset caricato! Righe: {len(train_df)}\")\n",
        "print(f\"Esempio Summary: {train_df['summary'].iloc[0]}\")\n"
      ],
      "metadata": {
        "id": "iS-FLQUN6alP",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# 4. Vocabulary and Tokenization\n",
        "# ============================================================\n",
        "\n",
        "#Definizione delle parole speciali\n",
        "SPECIAL_TOKENS = {\n",
        "    \"<pad>\": 0,       #per la normalizzazione della lunghezza\n",
        "    \"<sos>\": 1,       #start of sequence\n",
        "    \"<eos>\": 2,       #end of sequence\n",
        "    \"<unk>\": 3        #per parole assenti dal vocabolario\n",
        "}\n",
        "\n",
        "#Libreria per le espressioni regolari per la tokenizzazione del codice\n",
        "import re\n",
        "\n",
        "# tokenizzazione delle frasi usando parole\n",
        "# e qualsiasi tipo di carattere speciale diverso\n",
        "# dallo spazio come input diviso\n",
        "def tokenize(sentence):\n",
        "    sentence = sentence.lower()\n",
        "    tokens = re.findall(r\"\\w+|[^\\w\\s]\", sentence)\n",
        "    return tokens\n",
        "\n",
        "# Creazione del vocabolario con tutte le parole speciali e tutte\n",
        "# quelle presenti nell'input di training\n",
        "def build_vocab(sentences):\n",
        "    vocab = dict(SPECIAL_TOKENS)\n",
        "    for sent in sentences:\n",
        "        for token in tokenize(sent):\n",
        "            if token not in vocab:\n",
        "                vocab[token] = len(vocab)\n",
        "    return vocab\n",
        "\n",
        "#Creazione di due vocabolari distinti per il codice e per il linguaggio naturale\n",
        "code_vocab = build_vocab(train_df['code'])\n",
        "summary_vocab = build_vocab(train_df['summary'])\n",
        "\n",
        "# Encoding delle sequenze in input in numeri secondo i vocabolari appena creati\n",
        "def encode(sentence, vocab, max_len):\n",
        "    tokens = tokenize(sentence)\n",
        "    ids = [vocab.get(tok, vocab[\"<unk>\"]) for tok in tokens]\n",
        "    ids = [vocab[\"<sos>\"]] + ids + [vocab[\"<eos>\"]]\n",
        "    return pad_sequence(ids, max_len, vocab[\"<pad>\"])\n",
        "\n",
        "#Creazione dei tensor che serviranno per il machine learning\n",
        "print(\"Encoding del codice...\")\n",
        "X = torch.tensor([encode(c, code_vocab, MAX_CODE_LEN) for c in tdqm(train_df['code'])]).to(DEVICE)\n",
        "print(\"Encoding del summary...\")\n",
        "Y = torch.tensor([encode(s, summary_vocab, MAX_SUMMARY_LEN) for s in tdqm(train_df['summary'])]).to(DEVICE)\n"
      ],
      "metadata": {
        "id": "dLnUExPC6e3q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# 5. Model Definition\n",
        "# ============================================================\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, vocab_size):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, EMBEDDING_DIM)\n",
        "        self.lstm = nn.LSTM(EMBEDDING_DIM, HIDDEN_DIM, batch_first=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        emb = self.embedding(x)\n",
        "        # encoder_outputs contiene tutte le informazione per l'attention\n",
        "        encoder_outputs, (h, c) = self.lstm(emb)\n",
        "        return encoder_outputs, h, c\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, vocab_size, attention):\n",
        "        super().__init__()\n",
        "        self.attention = attention\n",
        "        self.embedding = nn.Embedding(vocab_size, EMBEDDING_DIM)\n",
        "        # L'input dell'LSTM ora riceve Embedding + Vettore di Contesto\n",
        "        self.lstm = nn.LSTM(HIDDEN_DIM + EMBEDDING_DIM, HIDDEN_DIM, batch_first=True)\n",
        "        self.fc = nn.Linear(HIDDEN_DIM * 2 + EMBEDDING_DIM, vocab_size)\n",
        "\n",
        "    def forward(self, x, hidden, cell, encoder_outputs):\n",
        "        # x: [batch_size, 1]\n",
        "        emb = self.embedding(x) # [batch, 1, emb_dim]\n",
        "\n",
        "        # 1. Calcolo dei pesi di attenzione\n",
        "        a = self.attention(hidden, encoder_outputs) # [batch, src_len]\n",
        "        a = a.unsqueeze(1) # [batch, 1, src_len]\n",
        "\n",
        "        # 2. Creazione del vettore di contesto (media pesata degli output encoder)\n",
        "        weighted = torch.bmm(a, encoder_outputs) # [batch, 1, hidden_dim]\n",
        "\n",
        "        # 3. Concatenazione di input + contesto per l'LSTM\n",
        "        rnn_input = torch.cat((emb, weighted), dim=2)\n",
        "        out, (hidden, cell) = self.lstm(rnn_input, (hidden, cell))\n",
        "\n",
        "        # 4. Predizione finale usando out, contesto e embedding originale\n",
        "        prediction = self.fc(torch.cat((out, weighted, emb), dim=2))\n",
        "\n",
        "        return prediction.squeeze(1), hidden, cell\n",
        "\n",
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder, device):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.device = device\n",
        "\n",
        "    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n",
        "        batch_size = trg.shape[0]\n",
        "        trg_len = trg.shape[1]\n",
        "        trg_vocab_size = self.decoder.fc.out_features\n",
        "\n",
        "        outputs = torch.zeros(batch_size, trg_len, trg_vocab_size).to(self.device)\n",
        "\n",
        "        # Otteniamo tutti gli output dall'encoder\n",
        "        encoder_outputs, hidden, cell = self.encoder(src)\n",
        "\n",
        "        input = trg[:, 0]\n",
        "\n",
        "        for t in range(1, trg_len):\n",
        "            # Passiamo encoder_outputs ad ogni step\n",
        "            output, hidden, cell = self.decoder(input.unsqueeze(1), hidden, cell, encoder_outputs)\n",
        "            outputs[:, t, :] = output\n",
        "\n",
        "            top1 = output.argmax(1)\n",
        "            teacher_force = random.random() < teacher_forcing_ratio\n",
        "            input = trg[:, t] if teacher_force else top1\n",
        "\n",
        "        return outputs\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, hidden_dim):\n",
        "        super().__init__()\n",
        "        # Strati per calcolare l'energia (importanza) tra decoder e encoder\n",
        "        self.attn = nn.Linear(hidden_dim * 2, hidden_dim)\n",
        "        self.v = nn.Linear(hidden_dim, 1, bias=False)\n",
        "\n",
        "    def forward(self, hidden, encoder_outputs):\n",
        "        # hidden: [1, batch_size, hidden_dim] (stato attuale decoder)\n",
        "        # encoder_outputs: [batch_size, src_len, hidden_dim]\n",
        "\n",
        "        batch_size = encoder_outputs.shape[0]\n",
        "        src_len = encoder_outputs.shape[1]\n",
        "\n",
        "        # Ripetiamo l'hidden state per ogni parola del codice in input\n",
        "        hidden = hidden.repeat(src_len, 1, 1).transpose(0, 1) # [batch, src_len, hidden_dim]\n",
        "\n",
        "        # Calcolo dell'allineamento (energia)\n",
        "        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim=2)))\n",
        "        attention = self.v(energy).squeeze(2) # [batch, src_len]\n",
        "\n",
        "        # Softmax per avere pesi che sommano a 1\n",
        "        return torch.softmax(attention, dim=1)\n"
      ],
      "metadata": {
        "id": "LTkHJZCN6hwX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# 6. Training Loop\n",
        "# ============================================================\n",
        "\n",
        "#Import necessario per il dataloader\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "#Usiamo il dataloader per non passare tutto il dataset in una volta\n",
        "#Che potrebbe risultare in impossibilità di computazione per la CPU\n",
        "dataset = TensorDataset(X, Y)\n",
        "train_loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "#Questa variabile ci servirà per plottare la loss\n",
        "train_losses = []\n",
        "\n",
        "#Inizializzazione dell'encoder e del decoder, optimizer e loss\n",
        "attn = Attention(HIDDEN_DIM)\n",
        "enc = Encoder(len(code_vocab)).to(DEVICE)\n",
        "dec = Decoder(len(summary_vocab), attn).to(DEVICE)\n",
        "model = Seq2Seq(enc, dec, DEVICE).to(DEVICE)\n",
        "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=0) #ignora il padding\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "\n",
        "    for batch_x, batch_y in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Ora passiamo tutto al modello Seq2Seq.\n",
        "        # Lui si occuperà di gestire l'encoder, il decoder e il teacher forcing internamente.\n",
        "        # batch_y[:, :-1] sono gli input per il decoder (<sos> + parole)\n",
        "        # batch_y[:, 1:]  sono i target che vogliamo predire (parole + <eos>)\n",
        "        outputs = model(batch_x, batch_y)\n",
        "\n",
        "        # Il primo token dell'output (tempo 0) è zero o non usato,\n",
        "        # confrontiamo dal tempo 1 in poi\n",
        "        output_dim = outputs.shape[-1]\n",
        "\n",
        "        # Flattening per la CrossEntropy:\n",
        "        # [batch, seq_len, vocab] -> [batch * seq_len, vocab]\n",
        "        loss = criterion(\n",
        "            outputs[:, 1:, :].reshape(-1, output_dim),\n",
        "            batch_y[:, 1:].reshape(-1)\n",
        "        )\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    avg_loss = epoch_loss / len(train_loader)\n",
        "    train_losses.append(avg_loss) #Salviamo la loss media di questo epoch\n",
        "    print(f\"Epoch {epoch+1}/{NUM_EPOCHS} - Loss: {epoch_loss/len(train_loader):.4f}\")\n",
        "\n",
        "# Calcoliamo la perplexity per ogni epoca dalle loss salvate\n",
        "train_perplexity = [np.exp(l) for l in train_losses]\n",
        "\n",
        "fig, ax1 = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "# Primo asse: Loss\n",
        "color = 'royalblue'\n",
        "ax1.set_xlabel('Epoca', fontsize=12)\n",
        "ax1.set_ylabel('Loss (Errore)', fontsize=12, color=color)\n",
        "ax1.plot(train_losses, color=color, linewidth=2, label='Training Loss')\n",
        "ax1.tick_params(axis='y', labelcolor=color)\n",
        "ax1.grid(True, linestyle='--', alpha=0.3)\n",
        "\n",
        "# Secondo asse: Perplexity\n",
        "ax2 = ax1.twinx() # Crea un secondo asse che condivide lo stesso asse X\n",
        "color = 'crimson'\n",
        "ax2.set_ylabel('Perplexity', fontsize=12, color=color)\n",
        "ax2.plot(train_perplexity, color=color, linewidth=2, linestyle='--', label='Perplexity')\n",
        "ax2.tick_params(axis='y', labelcolor=color)\n",
        "\n",
        "# Titolo e legenda\n",
        "plt.title('Andamento dell\\'Addestramento: Loss e Perplexity', fontsize=14)\n",
        "fig.tight_layout() # Per evitare sovrapposizioni tra le etichette degli assi\n",
        "\n",
        "# Uniamo le legende dei due assi\n",
        "lines, labels = ax1.get_legend_handles_labels()\n",
        "lines2, labels2 = ax2.get_legend_handles_labels()\n",
        "ax1.legend(lines + lines2, labels + labels2, loc='upper right')\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "UDR_PxC-6j8-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# 7. Inference / Code Summarization\n",
        "# ============================================================\n",
        "\n",
        "def summarize_code(model, code_sentence, src_vocab, tgt_vocab, max_len=MAX_SUMMARY_LEN):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        # 1. Preprocessing del codice in input\n",
        "        tokens = tokenize(code_sentence)\n",
        "        ids = [src_vocab.get(tok, src_vocab[\"<unk>\"]) for tok in tokens]\n",
        "        ids = [src_vocab[\"<sos>\"]] + ids + [src_vocab[\"<eos>\"]]\n",
        "        src_tensor = torch.tensor(ids).unsqueeze(0).to(DEVICE) # [1, seq_len]\n",
        "\n",
        "        # 2. Encoder: otteniamo TUTTI gli output per l'attenzione\n",
        "        encoder_outputs, hidden, cell = model.encoder(src_tensor)\n",
        "\n",
        "        # 3. Preparazione generazione\n",
        "        inputs = [tgt_vocab[\"<sos>\"]]\n",
        "\n",
        "        for i in range(max_len):\n",
        "            input_tensor = torch.tensor([inputs[-1]]).to(DEVICE) # Ultimo token generato\n",
        "\n",
        "            # 4. Decoder con Attention\n",
        "            # Passiamo anche encoder_outputs\n",
        "            output, hidden, cell = model.decoder(\n",
        "                input_tensor.unsqueeze(0),\n",
        "                hidden,\n",
        "                cell,\n",
        "                encoder_outputs\n",
        "            )\n",
        "\n",
        "            # Prendiamo la parola più probabile\n",
        "            predicted_id = output.argmax(1).item()\n",
        "            inputs.append(predicted_id)\n",
        "\n",
        "            # Se il modello decide di chiudere la frase\n",
        "            if predicted_id == tgt_vocab[\"<eos>\"]:\n",
        "                break\n",
        "\n",
        "        # 5. Traduzione da ID a parole\n",
        "        inv_vocab = {v: k for k, v in tgt_vocab.items()}\n",
        "        summary_tokens = [inv_vocab.get(idx, \"<unk>\") for idx in inputs\n",
        "                          if idx not in [SPECIAL_TOKENS[\"<sos>\"], SPECIAL_TOKENS[\"<eos>\"], SPECIAL_TOKENS[\"<pad>\"]]]\n",
        "\n",
        "        return \" \".join(summary_tokens)\n",
        "\n",
        "# Creiamo un dizionario inverso per decodificare (ID -> Parola)\n",
        "tgt_vocab_inv = {v: k for k, v in summary_vocab.items()}\n",
        "\n",
        "#================================================================\n",
        "# Confronto tra frasi attese e frasi prodotte\n",
        "#================================================================\n",
        "def run_test_samples(n=5):\n",
        "    print(f\"{'CODICE ORIGINALE':<50} | {'RIASSUNTO REALE':<30} | {'GENERATO'}\")\n",
        "    print(\"-\" * 130)\n",
        "\n",
        "    # Usiamo train_df (o val_df se l'hai creato) per recuperare i testi\n",
        "    for i in range(n):\n",
        "        idx = random.randint(0, len(train_df)-1)\n",
        "        original_code = train_df.iloc[idx]['code']\n",
        "        real_summary = train_df.iloc[idx]['summary']\n",
        "\n",
        "        generated_summary = summarize_code(model, original_code, code_vocab, summary_vocab)\n",
        "\n",
        "        # Visualizzazione formattata\n",
        "        short_code = (original_code[:47] + '...') if len(original_code) > 47 else original_code\n",
        "        short_real = (real_summary[:27] + '...') if len(real_summary) > 27 else real_summary\n",
        "\n",
        "        print(f\"{short_code:<50} | {short_real:<30} | {generated_summary}\")\n",
        "\n",
        "# Eseguiamo il test\n",
        "run_test_samples(n=5)\n"
      ],
      "metadata": {
        "id": "zmyjP24Q6pow"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# 8. Evaluation Metrics\n",
        "# ============================================================\n",
        "\n",
        "#=============================================================\n",
        "# BLEU & ROGUE SCORE\n",
        "#=============================================================\n",
        "from tqdm.auto import tqdm\n",
        "from rouge_score import rouge_scorer\n",
        "\n",
        "def calculate_metrics_complete(model, test_df, src_vocab, tgt_vocab, n_samples=100):\n",
        "    model.eval()\n",
        "    scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
        "\n",
        "    bleu_scores = []\n",
        "    rouge_l_scores = []\n",
        "\n",
        "    samples = test_df.sample(n=min(n_samples, len(test_df)))\n",
        "\n",
        "    for _, row in tqdm(samples.iterrows(), total=len(samples)):\n",
        "        gen_sum = summarize_code(model, row['code'], src_vocab, tgt_vocab)\n",
        "        real_sum = row['summary']\n",
        "\n",
        "        # BLEU (quello che abbiamo già)\n",
        "        ref = [tokenize(real_sum)]\n",
        "        cand = tokenize(gen_sum)\n",
        "        bleu_scores.append(sentence_bleu(ref, cand, smoothing_function=SmoothingFunction().method1))\n",
        "\n",
        "        # ROUGE-L\n",
        "        scores = scorer.score(real_sum, gen_sum)\n",
        "        rouge_l_scores.append(scores['rougeL'].fmeasure)\n",
        "\n",
        "    print(f\"BLEU Medio: {sum(bleu_scores)/len(bleu_scores):.4f}\")\n",
        "    print(f\"ROUGE-L Medio: {sum(rouge_l_scores)/len(rouge_l_scores):.4f}\")\n",
        "\n",
        "avg_score = calculate_metrics_complete(model, train_df, code_vocab, summary_vocab, n_samples=100)\n",
        "\n",
        "#=============================================================\n",
        "# PERPLEXITY BASED ON TRAINING LAST LOSS\n",
        "#=============================================================\n",
        "# Per la Perplessità (usa l'ultima loss calcolata nel training):\n",
        "import math\n",
        "final_perplexity = math.exp(train_losses[-1])\n",
        "print(f\"Perplexity Finale: {final_perplexity:.4f}\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "CCZs5AVf6mKt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# 9. Main Execution\n",
        "# ============================================================\n",
        "\n",
        "print(\"Project execution completed.\")\n"
      ],
      "metadata": {
        "id": "s5X_s7za6sGI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}